MLLM Datasets,Type,Language,Time,Train,Dev,Test,Total,Model,Application,Discription,Reference,Cite,Resource
MIMIC-III,EHR,EN,2016,--,--,--,"61,293",--,--,"MIMIC-III ('Medical Information Mart for Intensive Care') is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more.","MIMIC-III, a freely accessible critical care database",7588,https://physionet.org/content/mimiciii/1.4/
MIMIC-IV,EHR,EN,2023,--,--,--,85k,--,--,"Medical Information Mart for Intensive Care (MIMIC)-IV, a large deidentified dataset of patients admitted to the emergency department or an intensive care unit at the Beth Israel Deaconess Medical Center in Boston, MA. MIMIC-IV contains data for over 65,000 patients admitted to an ICU and over 200,000 patients admitted to the emergency department. MIMIC-IV incorporates contemporary data and adopts a modular approach to data organization, highlighting data provenance and facilitating both individual and combined use of disparate data sources. MIMIC-IV is intended to carry on the success of MIMIC-III and support a broad set of applications within healthcare.","MIMIC-IV, a freely accessible electronic health record dataset",677,https://physionet.org/content/mimiciv/3.1/
CPRD,EHR,EN,2015,--,--,--,"11,299,221",--,--,"The Clinical Practice Research Datalink (CPRD) is an ongoing primary care database of anonymised medical records from general practitioners, with coverage of over 11.3 million patients from 674 practices in the UK. With 4.4 million active (alive, currently registered) patients meeting quality criteria, approximately 6.9% of the UK population are included and patients are broadly representative of the UK general population in terms of age, sex and ethnicity. General practitioners are the gatekeepers of primary care and specialist referrals in the UK. The CPRD primary care database is therefore a rich source of health data for research, including data on demographics, symptoms, tests, diagnoses, therapies, health-related behaviours and referrals to secondary care. For over half of patients, linkage with datasets from secondary care, disease-specific cohorts and mortality records enhance the range of data available for research.",Data Resource Profile: Clinical Practice Research Datalink (CPRD),2709,https://academic.oup.com/ije/article/44/3/827/632531
OpenI,EHR & Multimodal,EN,2015,--,--,--,"3,996+8121",--,--,"an approach to developing a collection of radiology examinations, including both the images and radiologist narrative reports. The authors collected 3996 radiology reports from the Indiana Network for Patient Care and 8121 associated images from the hospitals' picture archiving systems.",Preparing a collection of radiology examinations for distribution and retrieval,915,http://openi.nlm.nih.gov/
PubMed,Literature,EN,--,--,--,--,--,--,--,,--,--,https://pubmed.ncbi.nlm.nih.gov/
PMC,Literature,EN,--,--,--,--,--,--,--,,--,--,https://www.ncbi.nlm.nih.gov/pmc/
CORD-19,Literature,EN,2020,--,--,--,--,--,--,,CORD-19: The COVID-19 Open Research Dataset,956,https://www.kaggle.com/datasets/allen-institute-for-ai/CORD-19-research-challenge
PubMedQA,QA,EN,2019,,500,500,273.2k,--,--,"a biomedical QA dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/-maybe, which can be considered as the multiple-choice question. It is split into three subsets: 1k manually labeled pairs (PQA-L), 61.2k unlabeled pairs (PQA-U), and 211.3k artificially generated pairs (PQA-A). Following former works (Diao, Pan et al. 2023), we view PQAA as the train set, PQA-L as the test set, and discard the PQA-U parts.",PubMedQA: A Dataset for Biomedical Research Question Answering,558,https://pubmedqa.github.io
MedQA (USMLE),QA,EN & ZH,2020,--,--,--,60k,--,--,"a dataset of multiple choice questions (4 choices per question), based on the United States Medical License Exams. The dataset is collected from the professional medical board exams, covering three languages: English, simplified Chinese, and traditional Chinese, containing 12,724, 34,251, and 14,123 questions respectively. Here, we use the English parts and split it into 10,178 questions for training, 1273 for validation, and 1273 for testing, following the official splits.",What Disease Does This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams,376,https://github.com/jind11/MedQA
MedMCQA,QA,EN,2022,--,187k,6.1k,193k,--,--,"a dataset of multiple choice questions, that are sourced from mock exams and past exams of two Indian medical school entrance exams called AIIMS and NEET-PG (Pal, Umapathi et al. 2022). The train split contains 182,822 questions, and the test split contains 4183 questions. Each question has 4 choices.",MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering,274,medmcqa.github.io
cMedQA2,QA,ZH,2018,--,--,--,108k,biGRU,Pre-train,"the dataset consists of 3 parts: training set, development set and test set. The training set is used to train the model, the development set is used to tune the hyper-parameters in models, and the test set is used to evaluate different models. The average character number of questions is about 49, and that of answers is about 101.",Multi-Scale Attentive Interaction Networks for Chinese Medical Question Answer Selection,68,https://github.com/zhangsheng93/cMedQA2
MultiMedQA,QA,EN,2022,--,--,--,--,Med-PaLM,Evaluation,"a benchmark combining six existing open question answering datasets spanning professional medical exams, research, and consumer queries",Large Language Models Encode Clinical Knowledge,1281,https://huggingface.co/datasets/openlifescienceai/multimedqa
HealthSearchQA,QA,EN,2022,--,--,--,"3,375",--,--,"We curated our own additional dataset consisting of 3375 commonly searched consumer questions, referred to as ""HealthSearchQA"". The dataset was curated using seed medical conditions and their associated symptoms. We used the seed data to retrieve publicly-available commonly searched questions generated by a search engine, which were displayed to all users entering the seed terms. We publish the dataset as an open benchmark for consumer medical question answering and hope this will be a useful resource for the community, as a dataset reflecting real-world consumer concerns.",Large Language Models Encode Clinical Knowledge,1281,https://opendatalab.com/OpenDataLab/HealthSearchQA
MedicationQA,QA,EN,2019,--,--,674,674,--,--,"The MedicationQA dataset consists of commonly asked consumer questions about medications. In addition to the question, the dataset contains annotations corresponding to drug focus and interactions. Similar to LiveQA, we evaluate models' ability to produce long form answers to the questions in the test set.",Bridging the Gap Between Consumers' Medication Questions and Trusted Answers,50,https://github.com/abachaa/Medication_QA_MedInfo2019
MMLU,QA,EN,2020,--,123,"1,089","1,212",--,--,"includes exam questions from 57 domains. We selected the subtasks most relevant to medical knowledge: ""anatomy"", ""clinical knowledge"", ""college medicine"", ""medical genetics"", ""professional medicine"", and ""college biology"". Each MMLU subtask contains multiple-choice questions with four options, along with the answers.",MEASURING MASSIVE MULTITASKLANGUAGE UNDERSTANDING,1955,https://huggingface.co/datasets/cais/mmlu
LiveQA(TREC-2017),QA,EN,2017,--,634,104,738,--,--,The LiveQA dataset was curated as part of the Text Retrieval Challenge (TREC) 2017. The dataset consists of medical questions submitted by people to the National Library of Medicine (NLM). The dataset also consists of manually collected reference answers from trusted sources such as the National Institute of Health (NIH) website.,Overview of the Medical Question Answering Task at TREC 2017 LiveQA,76,https://github.com/abachaa/LiveQA_MedicalTask_TREC2017
MedQuAD,QA,EN,2019,--,--,--,"47,457",,,"A collection of 47,457 medical question-answer pairs with additional annotations, constructed from trusted sources such as NIH websites. We make this resource publicly available(Health articles)",A QUESTION-ENTAILMENT APPROACH TO QUESTION ANSWERING,172,https://github.com/abachaa/MedQuAD
clinical-QE,QA,EN,2016,--,--,--,"8,588",--,--,"was constructed using 4,655 clinical questions asked by family doctors",Recognizing Question Entailment for Medical Question Answering,75,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333286/
Medical Meadow,QA,EN,2023,--,--,--,"160,076",MedAlpaca,Fine-tune,"Medical Meadow consists of two main categories, a collection of established medical NLP tasks reformatted in instruction tuning formats as well as a crawl of various internet resources. Each dataset focuses on different aspects of medical knowledge and practice",MEDALPACA - AN OPEN-SOURCE COLLECTION OF MEDICALCONVERSATIONAL AI MODELS AND TRAINING DATA,188,https://huggingface.co/datasets/medalpaca/medical_meadow_medqa
Huatuo-26M,QA,ZH,2023,"26,239,047",--,"265,041","26,504,088",--,--,"We collect the largest medical QA dataset from various sources as below: (i) collected from an online medical consultation website; (ii) automatically extracted from medical encyclopedias, and (iii) automatically extracted from medical knowledge bases. After text cleaning and data deduplication, we obtained the largest Chinese medical QA dataset, containing 26 Million QA pairs.","Huatuo-26M, a Large-scale Chinese Medical QA Dataset",37,https://github.com/ FreedomIntelligence/Huatuo-26M
MedHop,QA,EN,2018,"1,620",342,546,"2,508",--,--,"Since both document complexity and number of documents per sample were significantly larger compared to WIKIHOP, it was not feasible to ask an annotator to read all support documents for 100 samples. We thus opted to verify the dataset quality by providing only the subset of documents relevant to support the correct answer, i.e., those traversed along the path reaching the answer. The annotator was asked if the answer to the query ""follows"", ""is likely"", or ""does not follow"", given the relevant documents. 68% of the cases were considered as ""follows"" or as ""is likely"". The majority of cases violating the distant supervision assumption were errors due to the lack of a necessary PPI in one of the connecting documents.(Drugs)",Constructing Datasets for Multi-hop Reading Comprehension Across Documents,564,https://opendatalab.com/OpenDataLab/MedHop
BiQA,QA,EN,2020,--,--,--,7.4k,TERM-PACRR,Pre-train,"We evaluated the BiQA corpus using two strategies. First, we compared the results obtained with three document search methods on the questions of the dataset. This way, we could compare the references obtained through user answers with the ones obtained directly using search engines. The scores obtained indicate if search engine techniques can obtain the same answers that were provided by the users. Secondly, we evaluated the dataset by applying it to a 6 biomedical question answering task. The objective was to determine if, with our dataset, we can obtain similar performance to a dataset developed by domain experts.",Generating Biomedical Question Answering Corpora from Q&A Forums,11,https://github.com/lasigeBioTM/BiQA
BioASQ6,QA,EN,2018,--,--,--,3k,--,--,,AUEB at BioASQ 6: Document and Snippet Retrieval,24,https://github.com/nlpaueb/aueb-bioasq6
MEDIQA,QA,EN,2019,--,--,--,208,--,--,"MEDIQA 2019 includes three tasks: Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and Question Answering (QA) in the medical domain. 72 teams participated in the challenge, achieving an accuracy of 98% in the NLI task, 74.9% in the RQE task, and 78.3% in the QA task. In this paper, we describe the tasks, the datasets, and the participants' approaches and results. We hope that this shared task will attract further research efforts in textual inference, question entailment, and question answering in the medical domain.","Overview of the?MEDIQA?2019 Shared Task on Textual Inference, Question Entailment and Question Answering",134,https://sites.google.com/view/mediqa2019
emrQA,QA,EN,2018,--,--,--,455k,--,--,"We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets¡ì. The resulting corpus (emrQA) has 1 million question-logical form and 400,000+ questionanswer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping.(Medical Records)",emrQA: A Large Corpus for Question Answering on Electronic Medical Records,198,https://emrqa.github.io/
emrKBQA,QA,EN,2021,--,--,--,"940,713",--,--,"a dataset for answering physician questions from a structured patient record. It consists of questions, logical forms and answers. The questions and logical forms are generated based on real-world physician questions and are slot-filled and answered from patients in the MIMIC-III KB (Johnson et al., 2016) through a semi-automated process. This community-shared release consists of over 940000 question, logical form and answer triplets with 389 types of questions and¡Ö7.5 paraphrases per question type.",emrKBQA: A Clinical Knowledge-Base Question Answering Dataset,29,https://github.com/emrqa/emrkbqa
HealthQA,QA,EN,2019,--,--,--,"7,517",HAR,Evaluation,"To create HealthQA dataset, we collected healthcare articles from the popular health-services website Patient. We scraped all the articles from the Health Topics section of Patient. The website contains articles from a diverse set of healthcare domains such as child health, mental health, sexual health, details about treatments and medications, and several other healthcare domains. The articles on this website are much more detailed, as compared to other healthcare knowledgebases like MedlinePlus3. In total, we collected 1,235 health articles, with each article having an average of 6 sections. As the sections themselves are very long in these articles, we use each section as one document.",A Hierarchical Attention Retrieval Model for Healthcare Question Answering,62,https://github.com/mingzhu0527/HAR
MASH-QA,QA,EN,2020,"27,728","3,493","3,587",35k,MultiCo,Evaluation,"Since we focus on the task of multi-span questionanswering from long documents, our dataset consists of (question, context, [answer sentences]) tuples. Each tuple consists of a natural language question, which can be answered using one or more sentences from the context. Context here is a long document, a typical web article with multiple paragraphs. Each answer consists of several sentences, which can either belong to one single span, or multiple spans from the context document. Since questions in our dataset can have multiple sentences that form the answer, we provide the index of all correct answer sentences with each tuple. We refer to the single-span answer subset of our dataset as MASH-QA-S, and the multi-span answer subset as MASH-QA-M.",Question Answering with Long Multiple-Span Answers,71,https://github.com/mingzhu0527/MASHQA
webMedQA,QA,ZH,2019,"50,610","6,337","6,337","63,284",MV-LSTM,Evaluation,Clinical Reports,Applying deep matching networks to Chinese medical question answering: a study and a dataset,46,https://github.com/hejunqing/webMedQA
CliCR,QA,EN,2018,"91,344","6,391","7,184",105k,--,--,based on clinical report summaries,CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension,102,https://github.com/clips/clicr
Psych8k,QA,EN,2023,--,--,--,"8,187",ChatCounselor,Fine-tune,"The training dataset, Psych8k, was constructed from 260 in-depth interviews, each spanning an hour. To assess the quality of counseling responses, the counseling Benchwas devised.",ChatCounselor: A Large Language Models for Mental Health Support,30,https://huggingface.co/datasets/EmoCareAI/Psych8k
PMC-VQA,QA & Multimodal,EN,2023,--,--,--,"226,946",MedVInT,Pre-train,"To facilitate the model training, we present a scalable pipeline for constructing PMC-VQA, a comprehensive MedVQA dataset comprising 227k VQA pairs across 149k images, spanning diverse modalities and diseases.",PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering,106,https://huggingface.co/datasets/xmcmic/PMC-VQA
VQA-RAD,QA & Multimodal,EN,2018,--,--,--,"3,515",--,--,"The final VQA-RAD dataset contains 3,515 total visual questions. Of these, 1,515 (43.1%) are free-form. 733 questions are rephrased which means 48.3% of the free-form questions have corresponding paraphrasing. The remaining 1,267 questions are framed and have corresponding free-form and rephrased questions. On average, there are 10 questions per image.",A dataset of clinically generated visual questions and answers about radiology images,265,https://huggingface.co/datasets/flaviagiammarino/vqa-rad
Slake,QA & Multimodal,EN & ZH,2021,"9,849","2,109","2,070",14k,--,--,"We select radiology images, covering healthy and unhealthy cases, from three open source datasets [7]1 [8]2 [9]3. From [8], we randomly select 179 chest X-Ray images and keep the original disease labels. From [7] and [9], we randomly choose 463 single-slice images from 3D volume cases. Then, experienced physicians label organs and diseases as detailed as possible with ITK-SNAP [10]4 as shown in Figure 1. In total, we annotate 642 images, including 12 diseases and 39 organs of the whole body. The diseases mainly include cancer (e.g., brain, liver, kidney, lung, etc.), and thoracic diseases (e.g., atelectasis, effusion, mass, pneumothorax, etc.). The images include 140 head CTs or MRIs, 41 neck CTs, 219 chest X-Rays or CTs, 201 abdomen CTs or MRIs, and 41 pelvic cavity CTs. The distribution is shown in Figure 2 (Left). Among these images, there are 282 CTs, 181 MRIs, and 179 X-Rays. All CTs and MRIs are axial single-slice. The number of images for each body part is set based on the complexity of the body part. For example, the number of diseases and organs in abdomen is much more than that in neck, so there are more images of abdomen than neck in the dataset.",SLAKE: A SEMANTICALLY-LABELED KNOWLEDGE-ENHANCED DATASET FOR MEDICAL VISUAL QUESTION ANSWERING,178,https://www.med-vqa.com/slake
PathVQA,QA & Multimodal,EN,2021,"17,325","9,462","6,012","32,799",--,--,"32,799 open-ended questions from 4,998 pathology images",PATHVQA: 30000+ QUESTIONS FOR MEDICAL VISUALQUESTION ANSWERING,138,https://huggingface.co/datasets/flaviagiammarino/path-vqa
VQA-Med,QA & Multimodal,EN,2021,"4,500",500,500,"5,500",--,--,"included a training set of 4,500 radiology images with 4,500 question-answer (QA) pairs (the same dataset used in 2020), a new validation set of 500 radiology images with 500 QA pairs, and a new test set of 500 radiology images with 500 questions about Abnormality.",Overview of the VQA-Med Task at ImageCLEF 2021: Visual Question Answering and Generation in the Medical Domain,93,https://github.com/abachaa/VQA-Med-2021
ChiMed-VL-Instruction,QA & Multimodal,ZH,2023,--,--,--,"469,441",Qilin-Med-VL,Fine-tune,"ChiMed-VL-Instruction comprises 469,441 question-answer pairs. Within this subset, the questions section contains 10M tokens with a median length of 20 (Q1: 16, Q3: 25), posing a concise inquiry reflective of medical queries. The answers consist of 13M tokens with a median length slightly longer at 22 (Q1: 12, Q3: 34), providing clear, direct, and informative responses.",Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare,28,https://github.com/williamliujl/Qilin-Med-VL
ChiMed-VL-Alignment,Multimodal,ZH,2023,--,--,--,"580,014",Qilin-Med-VL,Fine-tune,"ChiMed-VL-Alignment consists of 580,014 image-text couplings, each pair falling into one of two categories: context information of an image or descriptions of an image. The context category contains 167M tokens, presenting a median text length of 435 (Q1: 211, Q3: 757). Conversely, descriptions, more concise and image-specific, contain inline descriptions and captions. They comprise 63M tokens, with median lengths settling at 59 (Q1: 45, Q3: 83).",Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare,28,https://github.com/williamliujl/Qilin-Med-VL
PMC-CaseReport,QA & Multimodal,EN,2023,"316,838",--,"120,836","437,674",--,--,317K VQA pairs for taining and of 121K for testing images,--,--,https://huggingface.co/datasets/chaoyi-wu/PMC-CaseReport
MedC-K,Dialogue,EN,2024,--,--,--,79B tokens,PMC-LLaMA,Pre-train,"As a valuable knowledge resource, academic papers naturally contains high-quality, cutting-edge medical knowledge. We start with the S2ORC (Lo et al. 2020) Datasets with 81.1M English-language academic papers, and pick out those biomedical related papers depending on whether having corresponding PubMed Central (PMC) IDs. As a result, there are around 4.8M biomedical papers left, totaling over 75B tokens. We collect 30K textbooks sourced from various outlets, for example, the open-library, university library, and reputable publishers, covering a wide range of medical specialties as shown in Fig. 3. For preprocessing, we first extract the text content from the book PDF, then carry out data cleaning via de-duplication and content filtering. Specifically, we eliminate extraneous elements such as URLs, author lists, superfluous information, document contents, references, and citations. Additionally, we have also removed any references to images and tables within the paragraphs, for example, 'Fig. 1'. After this thorough cleaning process, there are approximately 4B tokens left.",PMC-LLaMA: Towards Building Open-source Language Models for Medicine,108,https://github.com/chaoyi-wu/PMC-LLaMA
MedC-I,QA & Instructions,EN,2024,--,--,--,202M tokens,PMC-LLaMA,Fine-tune,"systematically investigate the process of adapting a general-purpose foundation language model towards medical domain, this involves data-centric knowledge injection through the integration of 4.8M biomedical academic papers and 30K medical textbooks, as well as comprehensive fine-tuning for alignment with domain-specific instructions; a large-scale, comprehensive dataset for instruction tuning. This dataset encompasses medical question-answering (QA), rationale for reasoning, and conversational dialogues, comprising a total of 202M tokens;",PMC-LLaMA: Towards Building Open-source Language Models for Medicine,108,https://github.com/chaoyi-wu/PMC-LLaMA
CMtMedQA,QA & Dialogue,ZH,2024,"68,023",--,"1,000",70k,Zhongjing,Evaluation,"contains about 70,000 multi-turn dialogues and 400,000 conversations, It covers 14 medical departments and over 10 medical Q&A scenarios, such as disease diagnosis, medication advice, health consultation, medical knowledge, etc.",Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue,408,https://github.com/SupritYoung/Zhongjing
MedInstruct-52k,Instructions,EN,2023,52k,--,216,52k,AlpaCare,Fine-tune,"MedInstruct-52K, a diverse medical IFT dataset comprising 52K instructionresponse pairs and, MedInstruct-test, a test set of 216 clinician-crafted novel medical tasks, to facilitate the building and evaluation of medical LLMs.",ALPACARE: INSTRUCTION FINE-TUNED LARGELANGUAGE MODELS FOR MEDICAL APPLICATIONS,33,https://github.com/XZhang97666/AlpaCare
ChiMed,Multiple,ZH,2023,--,--,--,47K,Qilin-Med,Fine-tune,"ChineseMedicine (ChiMed) dataset, encompassing medical question answering, plain texts, knowledge graphs, and dialogues, segmented into three training stages",Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model,21,https://github.com/yuanheTian/ChiMed
CMExam,QA,ZH,2023,"54,495","6,812","6,812","68,119",--,Evaluation,"1)Reliability and Authenticity: CMExam is sourced exclusively from the CNMLE that undergoes rigorous review and validation processes, ensuring its accuracy and adherence to established medical standards. 2) Standardization and Comprehensiveness: CMExam includes both multiple-choice questions that ensure fair and objective evaluations of models' performance and question-wise open-ended reasoning that allows in-depth analysis and assessment of model reasoning abilities and comprehension. Despite the inherent absence of explanations within the CNMLE, we cross-referenced exam questions with solutions offered by diverse online medical examination preparation platforms, effectively enhancing the dataset's informational depth. CMExam reflects the comprehensive coverage of medical knowledge and reasoning required in clinical practice, as it is sourced from carefully designed national medical exams. The inclusion of five additional annotation dimensions enhances the dataset's rigor and offers valuable insights for in-depth evaluation and analysis. 3) Scale: CMExam consists of over 60K high-quality questions, providing a large and reliable dataset.",Benchmarking Large Language Models on CMExam - A Comprehensive Chinese Medical Exam Dataset,43,https://github.com/williamliujl/CMExam
MedDialog,Dialogue,EN & ZH,2020,--,--,--,"4,474,241",--,--,MedDialog-EN and MedDialog-CN. MedDialog-EN is an English dataset containing 0.3 million conversations between patients and doctors and 0.5 million utterances. MedDialog-CN is an Chinese dataset containing 1.1 million conversations and 4 million utterances.,MedDialog: Large-scale Medical Dialogue Datasets,160,https://github.com/UCSDAI4H/Medical-Dialogue-System
GAP-Replay,Multiple,EN,2023,21.1M,631K,--,21.7M,MEDITRON,Pre-train,"MEDITRON 's domain-adaptive pre-training corpus GAP-REPLAY combines 48.1B tokens from four datasets; Clinical G uidelines: a new dataset of 46K clinical practice guidelines from various healthcare-related sources, Paper A bstracts: openly available abstracts from 16.1M closed-access PubMed and PubMed Central papers, Medical Papers: full-text articles extracted from 5M publicly available PubMed and PubMed Central papers, and a Replay dataset: general domain data distilled to compose 1% of the entire corpus.",MEDITRON-70B: Scaling Medical Pretraining for Large Language Models,152,https://github.com/epfLLM/meditron
HealthCareMagic-100k,Dialogue,EN,2023,--,--,--,100k,ChatDoctor,Fine-tune,100k real conversations between patients and doctors from HealthCareMagic.com,ChatDoctor: A Medical Chat Model Fine-Tuned ona Large Language Model Meta-AI (LLaMA) UsingMedical Domain Knowledge,251,https://github.com/Kent0n-Li/ChatDoctor
icliniq-10k,Dialogue,EN,2023,--,--,--,10k,ChatDoctor,Evaluation,10k real conversations between patients and doctors from icliniq.com,ChatDoctor: A Medical Chat Model Fine-Tuned ona Large Language Model Meta-AI (LLaMA) UsingMedical Domain Knowledge,251,https://github.com/Kent0n-Li/ChatDoctor
GenMedGPT-5k,Dialogue,EN,2023,--,--,--,5k,--,--,5k generated conversations between patients and physicians from ChatGPT,ChatDoctor: A Medical Chat Model Fine-Tuned ona Large Language Model Meta-AI (LLaMA) UsingMedical Domain Knowledge,251,https://github.com/Kent0n-Li/ChatDoctor
UMLS,Knowledge Base,EN,2004,--,--,--,--,--,--,"The Uni?ed Medical Language System (http:// umlsks.nlm.nih.gov) is a repository of biomedical vocabularies developed by the US National Library of Medicine. The UMLS integrates over 2 million names for some 900 000 concepts from more than 60 families of biomedical vocabularies, as well as 12 million relations among these concepts. Vocabularies integrated in the UMLS Metathesaurus include the NCBI taxonomy, Gene Ontology, the Medical Subject Headings (MeSH), OMIM and the Digital Anatomist Symbolic Knowledge Base. UMLS concepts are not only inter-related, but may also be linked to external resources such as GenBank. In addition to data, the UMLS includes tools for customizing the Metathesaurus (MetamorphoSys), for generating lexical variants of concept names (lvg) and for extracting UMLS concepts from text (MetaMap). The UMLS knowledge sources are updated quarterly. All vocabularies are available at no fee for research purposes within an institution, but UMLS users are required to sign a license agreement. The UMLS knowledge sources are distributed on CD-ROM and by FTP.",The Unified Medical Language System (UMLS): integrating biomedical terminology,5212,http:// umlsks.nlm.nih.gov
CMeKG,Knowledge Base,ZH,2019,--,--,--,--,--,--,"We apply natural lan- guage processing and text mining techniques with a semi-automated approach to develop the Chinese Medical Knowl- edge Graph (CMeKG 1.0). The construction of CMeKG 1.0 refers to the international medical coding systems such as ICD-10£¬ATC£¬ and MeSH£¬as well as large-scale, multi-source heterogeneous clinical guidelines,medical stand- ards£¬diagnostic protocols, and medical encyclopedia resources.CMeKG covers types such as diseases,drugs,and diagnosis/treatment technologies,with more than l million medical concept relationships. This paper presents the description system, key technologies£¬construction process and medical knowledge description of CMeKG 1.0£¬ser- ving as a reference for the construction and application of knowledge graphs in the medical field.",Preliminary Study on the Construction of Chinese Medical Knowledge Graph,--,http://cmekg.pcl.ac.cn/
COMETA,Web Data,EN,2020,--,--,--,20k,Bioreddit,Pre-train,"COMETA, consisting of 20k English biomedical entity mentions from Reddit expert-annotated with links to SNOMED CT, a widely-used medical knowledge graph",COMETA: A Corpus for Medical Entity Linking in the Social Media,96,https://github.com/ cambridgeltl/cometa
?TCM-Corpus-1B,Dialogue,ZH,2024,--,--,--,20GB,TCM-GPT-7B,Pre-train,"The data utilized in this study primarily consists of three components: a general corpus, a TCM examination dataset (denoted as TCM-EXAM), and a TCM Electronic Health Record dataset (denoted as TCM-EHR). The general corpus was collected from Baidu Baike and Wikipedia sources. After undergoing a data cleaning process, this corpus encompasses a substantial volume of information, totally 20GB in size.",TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine,1,--
TCM-EXAM,QA,ZH,2024,"5,904",--,421,"6,325",TCM-GPT-7B,Fine-tune & Evaluation,"The TCM-EXAM dataset comprises TCM-specific examination questions from diverse sources. It encompasses a total of 6,325 multiple-choice questions. Within this dataset, we constructed a test subset that concentrates on questions related to Diagnostics (denoted as DT) and Formula Science (denoted as FS) in TCM, containing 214 and 207 questions, respectively. The remaining 5,904 questions are used as the train set.",TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine,1,--
TCM-EHR,EHR,ZH,2024,"7,483",--,300,"7,783",TCM-GPT-7B,Fine-tune & Evaluation,"The TCM-EHR dataset is collected from a traditional Chinese medicine hospital and encompasses 7,783 electronic health records. Each individual record encapsulates information including chief complaints, history of present illness, physical examinations, and diagnoses, among others. For this dataset, a targeted test subset was generated, including specific disease groups, namely ""Etiology and syndrome"" (denoted as ES) and ""Skin and mucosal diseases"" (denoted as SMD) within TCM. This test subset encompasses 150 and 150 records, while the remaining 7,483 records are used as the training set.",TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine,1,--
MIMIC-CXR,Multimodal,EN,2019,--,--,--,"377,110",--,--,"MIMIC-CXR, alarge dataset of 227,835 imaging studies for 65,379 patients presenting to the Beth Israel Deaconess Medical Center Emergency Department between 2011¨C2016. Each imaging study can contain one or more images, usually a frontal view and a lateral view. A total of 377,110 images are available in thedataset.","MIMIC-CXR, a de-identifiedpublicly available database of chest radiographs with free-text reports",997,https://physionet.org/content/mimic-cxr/2.1.0/
ROCO,Multimodal,EN,2018,--,--,--,"87,952",--,--,"The Radiology Objects in Context (ROCO) dataset has two classes: Radiology and Out-Of-Class. The first contains 81,825 radiology images with several medical imaging modalities including, Computer Tomography (CT), Ultrasound, X-Ray, Fluoroscopy, Positron Emission Tomography (PET), Mammography, Magnetic Resonance Imaging (MRI), Angiography and PET-CT. The latter contains 6,127 out-of-class samples, including synthetic radiology figures, digital art and portraits. The corresponding captions, keywords, UMLS (Unified Medical Language System) Semantic Types (SemTypes) [3], UMLS Concept Unique Identifiers (CUIs) [3] and download link is distributed for each image. Generative models trained on ROCO image - caption pairs can be used to automatically create natural sentences describing radiology images, as proposed in [14,23]. The keywords distributed can be adopted for multi-class classification tasks, semantic tagging and multi-modal image representation, as this has proven to obtain higher prediction results [15]. Each ROCO image has a ftp-download link, containing the figure name and PMC identifier, which can be used to extract the image and corresponding article.",Radiology Objects in COntext (ROCO): A Multimodal Image Dataset,215,https://github.com/razorx89/roco-dataset
OpenPath,Multimodal,EN,2023,--,--,--,"208,414",PLIP,Fine-tune,"we used the popular pathology Twitter hashtags to curate 243,375 public pathology images. We expanded this collection to include pathology data from other sites on the Internet (collected from the Large-scale Artificial Intelligence Open Network (LAION 22 )), followed by strict data quality filtering, finally creating a collection of 208,414 pathology image¨Ctext pairs called OpenPath.",A visual-language foundation model for pathology image analysis using medical Twitter,227,https://www.nature.com/articles/s41591-023-02504-3
MedICaT,Multimodal,EN,2020,--,--,--,"217,060",--,--,"MEDICAT, a dataset of medical images in context. MEDICAT consists of 217K images from 131K open access biomedical papers, and includes captions, inline references for 74% of figures, and manually annotated subfigures and subcaptions for a subset of figures.","MedICaT: A Dataset of Medical Images, Captions, and Textual References",61,https://github.com/allenai/medicat
CheXpert,Multimodal,EN,2019,--,--,--,"224,316",--,--,"CheXpert (Chest eXpert), a large dataset for chest radiograph interpretation. The dataset consists of 224,316 chest radiographs of 65,240 patients labeled for the presence of 14 common chest radiographic observations. We design a labeler that can extract observations from free-text radiology reports and capture uncertainties present in the reports by using an uncertainty label.",CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison,2640,https://stanfordmlgroup.github.io/competitions/chexpert
PathCap,Multimodal,EN,2023,--,--,--,142k,PathAsst,Fine-tune,"The PathCap contains 142K high quality pathology image-caption pairs. Among them, 132K are collected from sources such as PubMed, books, and pathology atlas websites, while an additional 10K annotations are provided by expert cytologists specializing in liquid-based cytology (LBC). Out of these, approximately 100K data gathered from the PubMed database will be made completely open-source.",PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology,23,https://github.com/superjamessyx/ Generative-Foundation-AI-Assistant-for-Pathology
PathInstruct,Multimodal & Instructions,EN,2023,--,--,--,180k,PathAsst,Fine-tune,"The PathInstruct dataset contains 180K samples encompasses two distinct part of instruction-following data. The first part is generated using ChatGPT/GPT-4, which builds on collected pathology image-text data. The second part consists of multimodal instruction data for model invocation, enabling the appropriate utilization of specialized pathology models in response to user intentions and image information.",PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology,23,https://github.com/superjamessyx/ Generative-Foundation-AI-Assistant-for-Pathology
MedMD,Multimodal,EN,2023,--,--,--,16M,RadFM,Pre-train,"Medical Multi-modalDataset, named MedMD, with around 16M medical scans in total, consisting of 15.5M 2D images and 180k 3D radiology scans (equivalent to 7M 2D slices) accompanied with high-quality textual descriptions, for example, radiology reports, visual-language instruction, or crucial disease diagnosis labels. MedMD encompasses a wide range of radiological modalities and anatomical regions of the human body, featuring over5000 diseases, thus can potentially serve as the cornerstone for developing foundation models in radiology.",Towards Generalist Foundation Model for Radiology,89,https://github.com/chaoyi-wu/RadFM
RadMD,Multimodal,EN,2023,--,--,--,3M,RadFM,Fine-tune,"is a radiologic cleaned version of MedMD, containing 3M radiologic visual-language pairs, termed as RadMD",Towards Generalist Foundation Model for Radiology,89,https://github.com/chaoyi-wu/RadFM
PMC-OA,Multimodal,EN,2023,--,--,--,165M,PMC-CLIP,Pre-train,"a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess subset, which is 8 times larger than before. PMC-OA covers diverse modalities or diseases, with majority of the image-caption samples aligned at finer-grained level,i.e., subfigure and subcaption.",PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents,82,https://github.com/WeixiongLin/PMC-CLIP
PMC-15M,Multimodal,EN,2023,13.9M,13.6k,726k,"15,282,336",BiomedCLIP,Fine-tune,This yields a dataset PMC-15M with 15 million figure-caption pairs from over 3 million articles.,LARGE-SCALE DOMAIN-SPECIFIC PRETRAINING FORBIOMEDICAL VISION-LANGUAGE PROCESSING,128,aka.ms/biomedclip
LLaVA-Med-Instruct,Multimodal & Instructions,EN,2023,--,--,--,600k,LLaVA-Med,Fine-tune,"We sample 600K image-text pairs from PMC-15M. Though this dataset only presents one-single task instructions, i.e., image captioning, it contains a diverse and representative set of biomedical concept samples from the original PMC-15M",LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day,372,https://aka.ms/llava-med
